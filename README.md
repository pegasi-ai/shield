# ğŸ›¡ï¸ Pegasi Shield 
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/release/python-370/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![PyPI - Python Version](https://img.shields.io/pypi/v/llm-guard)](https://pypi.org/project/guardrail-ml)
[![Downloads](https://static.pepy.tech/badge/guardrail-ml)](https://pepy.tech/project/guardrail-ml)
[![ICMLÂ 2025](https://img.shields.io/badge/ICML-2025-blue)](https://icml.cc/)

![plot](./static/images/safeguards-shield.png)

Pegasi Shield is a developer toolkit to use LLMs safely and securely. Our Shield safeguards prompts and LLM interactions from costly risks to bring your AI app from prototype to production faster with confidence.

Our Shield wraps your AI agents and GenAI with a protective layer, safeguarding malicious inputs and filtering model outputs. Our comprehensive toolkit has 20+ out-of-the-box detectors for robust protection of your GenAI apps in workflow. 

## Benefits
- ğŸš€ mitigate LLM reliability and safety risks 
- ğŸ“ customize and ensure LLM behaviors are safe and secure
- ğŸ’¸ access to model editing fine-tuned 4B models that is on par with OpenAI-o3

## Features 
- ğŸ› ï¸ shield that safeguards against costly risks like toxicity, bias, PI
- ğŸ¤– reduce and measure ungrounded additions (hallucinations) with tools
- ğŸ›¡ï¸ multi-layered defense with heuristic detectors, LLM-based, vector DB

Our research on hallucination detection and editing was peer-reviewed and accepted to ICML 2025 Workshop. Tutorials coming soon. 
